{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Local Research Assistant with Ollama and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain_openai langchain_core tavily-python langchain-community\n",
    "#Get the lastest version of Tavily for new extract feature\n",
    "\n",
    "%pip install --upgrade tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "import operator\n",
    "from dataclasses import dataclass, field, fields\n",
    "from typing_extensions import Annotated, Literal\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langsmith import traceable\n",
    "from tavily import TavilyClient\n",
    "from typing import Any, Optional\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"YOUR_TAVILY_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm=ChatOllama(model=\"deepseek-r1:latest\", temperature=0)#initialize model-specify which one you want to use, pull from ollama before\n",
    "llm_json_mode=ChatOllama(model=\"deepseek-r1:latest\", temperature=0, format=\"json\")#we generate structured outputs since this type of input needed for LangChain..\n",
    "#temperature=0 means you want to generate deterministic outputs-same input same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define our state..user defines research topic, the output is a summary, the research assistant needs other inputs and these\n",
    "#are defined in SummaryState\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryState:\n",
    "    research_topic: str = field(default=None) # The original topic the user wants to research   \n",
    "    search_query: str = field(default=None) # Search query\n",
    "    web_research_results: Annotated[list, operator.add] = field(default_factory=list) #Store results from web search\n",
    "    sources_gathered: Annotated[list, operator.add] = field(default_factory=list) \n",
    "    research_loop_count: int = field(default=0) # Research loop count-how many time re-queried\n",
    "    running_summary: str = field(default=None) # Final report\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryStateInput:\n",
    "    research_topic: str = field(default=None) # The input to our workflow- initial topic     \n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class SummaryStateOutput:\n",
    "    running_summary: str = field(default=None) # Final output expected from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n",
    "    \"\"\"\n",
    "    Takes either a single search response or list of responses from Tavily API and formats them.\n",
    "    Limits the raw_content to approximately max_tokens_per_source.\n",
    "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
    "    \n",
    "    Args:\n",
    "        search_response: Either:\n",
    "            - A dict with a 'results' key containing a list of search results\n",
    "            - A list of dicts, each containing search results\n",
    "            \n",
    "    Returns:\n",
    "        str: Formatted string with deduplicated sources\n",
    "    \"\"\"\n",
    "    # Convert input to list of results\n",
    "    if isinstance(search_response, dict):\n",
    "        sources_list = search_response['results']\n",
    "    elif isinstance(search_response, list):\n",
    "        sources_list = []\n",
    "        for response in search_response:\n",
    "            if isinstance(response, dict) and 'results' in response:\n",
    "                sources_list.extend(response['results'])\n",
    "            else:\n",
    "                sources_list.extend(response)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n",
    "    \n",
    "    # Deduplicate by URL\n",
    "    unique_sources = {}\n",
    "    for source in sources_list:\n",
    "        if source['url'] not in unique_sources:\n",
    "            unique_sources[source['url']] = source\n",
    "    \n",
    "    # Format output\n",
    "    formatted_text = \"Sources:\\n\\n\"\n",
    "    for i, source in enumerate(unique_sources.values(), 1):\n",
    "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
    "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
    "        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
    "        if include_raw_content:\n",
    "            # Using rough estimate of 4 characters per token\n",
    "            char_limit = max_tokens_per_source * 4\n",
    "            # Handle None raw_content\n",
    "            raw_content = source.get('raw_content', '')\n",
    "            if raw_content is None:\n",
    "                raw_content = ''\n",
    "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
    "            if len(raw_content) > char_limit:\n",
    "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
    "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
    "                \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def format_sources(search_results):\n",
    "    \"\"\"Format search results into a bullet-point list of sources.\n",
    "    \n",
    "    Args:\n",
    "        search_results (dict): Tavily search response containing results\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted string with sources and their URLs\n",
    "    \"\"\"\n",
    "    return '\\n'.join(\n",
    "        f\"* {source['title']} : {source['url']}\"\n",
    "        for source in search_results['results']\n",
    "    )\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the research assistant.\"\"\"\n",
    "    max_web_research_loops: int = 3\n",
    "    local_llm: str = \"llama3.2\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n",
    "#conditional edge..\n",
    "def route_research(state: SummaryState, config: RunnableConfig) -> Literal[\"finalize_summary\", \"web_research\"]:\n",
    "    \"\"\" Route the research based on the follow-up query \"\"\"\n",
    "\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    if state.research_loop_count <= configurable.max_web_research_loops:\n",
    "        return \"web_research\"\n",
    "    else:\n",
    "        return \"finalize_summary\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@traceable\n",
    "def tavily_search(query, include_raw_content=True, max_results=3):\n",
    "    \"\"\" Search the web using the Tavily API.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to execute\n",
    "        include_raw_content (bool): Whether to include the raw_content from Tavily in the formatted string\n",
    "        max_results (int): Maximum number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tavily search response containing:\n",
    "            - results (list): List of search result dictionaries, each containing:\n",
    "                - title (str): Title of the search result\n",
    "                - url (str): URL of the search result\n",
    "                - content (str): Snippet/summary of the content\n",
    "                - raw_content (str): Full content of the page if available\"\"\"\n",
    "     \n",
    "    tavily_client = TavilyClient()\n",
    "    return tavily_client.search(query, \n",
    "                         max_results=max_results, \n",
    "                         include_raw_content=include_raw_content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_writer_instructions=\"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic:\n",
    "{research_topic}\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
    "\n",
    "Your tasks:\n",
    "1. Identify knowledge gaps or areas that need deeper exploration\n",
    "2. Generate a follow-up question that would help expand your understanding\n",
    "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
    "\n",
    "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
    "\n",
    "Return your analysis as a JSON object:\n",
    "{{ \n",
    "    \"knowledge_gap\": \"string\",\n",
    "    \"follow_up_query\": \"string\"\n",
    "}}\"\"\"\n",
    "\n",
    "summarizer_instructions=\"\"\"Your goal is to generate a high-quality summary of the web search results.\n",
    "\n",
    "When EXTENDING an existing summary:\n",
    "1. Seamlessly integrate new information without repeating what's already covered\n",
    "2. Maintain consistency with the existing content's style and depth\n",
    "3. Only add new, non-redundant information\n",
    "4. Ensure smooth transitions between existing and new content\n",
    "\n",
    "When creating a NEW summary:\n",
    "1. Highlight the most relevant information from each source\n",
    "2. Provide a concise overview of the key points related to the report topic\n",
    "3. Emphasize significant findings or insights\n",
    "4. Ensure a coherent flow of information\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Start IMMEDIATELY with the summary content - no introductions or meta-commentary\n",
    "- DO NOT include ANY of the following:\n",
    "  * Phrases about your thought process (\"Let me start by...\", \"I should...\", \"I'll...\")\n",
    "  * Explanations of what you're going to do\n",
    "  * Statements about understanding or analyzing the sources\n",
    "  * Mentions of summary extension or integration\n",
    "- Focus ONLY on factual, objective information\n",
    "- Maintain a consistent technical depth\n",
    "- Avoid redundancy and repetition\n",
    "- DO NOT use phrases like \"based on the new results\" or \"according to additional sources\"\n",
    "- DO NOT add a References or Works Cited section\n",
    "- DO NOT use any XML-style tags like <think> or <answer>\n",
    "- Begin directly with the summary text without any tags, prefixes, or meta-commentary\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_query(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\" Generate a query for web search \"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic)\n",
    "\n",
    "    # Generate a query\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=query_writer_instructions_formatted),\n",
    "        HumanMessage(content=f\"Generate a query for web search:\")]\n",
    "    )   \n",
    "    query = json.loads(result.content)\n",
    "    \n",
    "    return {\"search_query\": query['query']}\n",
    "\n",
    "def web_research(state: SummaryState):\n",
    "    \"\"\" Gather information from the web \"\"\"\n",
    "    \n",
    "    # Search the web\n",
    "    search_results = tavily_search(state.search_query, include_raw_content=True, max_results=1)\n",
    "    \n",
    "    # Format the sources\n",
    "    search_str = deduplicate_and_format_sources(search_results, max_tokens_per_source=1000)\n",
    "    return {\"sources_gathered\": [format_sources(search_results)], \"research_loop_count\": state.research_loop_count + 1, \"web_research_results\": [search_str]}\n",
    "\n",
    "def summarize_sources(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\" Summarize the gathered sources \"\"\"\n",
    "    \n",
    "    # Existing summary\n",
    "    existing_summary = state.running_summary\n",
    "\n",
    "    # Most recent web research\n",
    "    most_recent_web_research = state.web_research_results[-1]\n",
    "\n",
    "    # Build the human message\n",
    "    if existing_summary:\n",
    "        human_message_content = (\n",
    "            f\"Extend the existing summary: {existing_summary}\\n\\n\"\n",
    "            f\"Include new search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "    else:\n",
    "        human_message_content = (\n",
    "            f\"Generate a summary of these search results: {most_recent_web_research} \"\n",
    "            f\"That addresses the following topic: {state.research_topic}\"\n",
    "        )\n",
    "\n",
    "    # Run the LLM\n",
    "\n",
    "    result = llm.invoke(\n",
    "        [SystemMessage(content=summarizer_instructions),\n",
    "        HumanMessage(content=human_message_content)]\n",
    "    )\n",
    "\n",
    "    running_summary = result.content\n",
    "\n",
    "    return {\"running_summary\": running_summary}\n",
    "\n",
    "def reflect_on_summary(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\" Reflect on the summary and generate a follow-up query \"\"\"\n",
    "\n",
    "    # Generate a query\n",
    "\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=reflection_instructions.format(research_topic=state.research_topic)),\n",
    "        HumanMessage(content=f\"Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}\")]\n",
    "    )   \n",
    "    follow_up_query = json.loads(result.content)\n",
    "\n",
    "    # Overwrite the search query\n",
    "    return {\"search_query\": follow_up_query['follow_up_query']}\n",
    "\n",
    "def finalize_summary(state: SummaryState):\n",
    "    \"\"\" Finalize the summary \"\"\"\n",
    "    \n",
    "    # Format all accumulated sources into a single bulleted list\n",
    "    all_sources = \"\\n\".join(source for source in state.sources_gathered)\n",
    "    state.running_summary = f\"## Summary\\n\\n{state.running_summary}\\n\\n ### Sources:\\n{all_sources}\"\n",
    "    return {\"running_summary\": state.running_summary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build the graph - Add nodes and edges\n",
    "\n",
    "builder = StateGraph(SummaryState, input=SummaryStateInput, output=SummaryStateOutput)\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_node(\"summarize_sources\", summarize_sources)\n",
    "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
    "builder.add_node(\"finalize_summary\", finalize_summary)\n",
    "\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"web_research\")\n",
    "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
    "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
    "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
    "builder.add_edge(\"finalize_summary\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "research_input=SummaryStateInput(\n",
    "    research_topic=\"Who developed Gemini?\"\n",
    ")\n",
    "summary=graph.invoke(research_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Markdown(summary['running_summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "prev_pub_hash": "cb5e5ac53cdc46d67cb32fd01f42a4d930d8a16d47ee21f17c804e4ab9ad587e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
